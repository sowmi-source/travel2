
==> Audit <==
|---------|--------------------------------|----------|-------|---------|---------------------|---------------------|
| Command |              Args              | Profile  | User  | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|-------|---------|---------------------|---------------------|
| start   | --embed-certs                  | minikube | deepa | v1.36.0 | 02 Sep 25 04:15 UTC |                     |
| start   |                                | minikube | deepa | v1.36.0 | 02 Sep 25 04:41 UTC |                     |
| delete  |                                | minikube | deepa | v1.36.0 | 02 Sep 25 05:11 UTC | 02 Sep 25 05:11 UTC |
| start   | --embed-certs                  | minikube | deepa | v1.36.0 | 02 Sep 25 05:11 UTC |                     |
| delete  |                                | minikube | deepa | v1.36.0 | 02 Sep 25 05:13 UTC | 02 Sep 25 05:13 UTC |
| start   | --embed-certs                  | minikube | deepa | v1.36.0 | 02 Sep 25 05:13 UTC |                     |
| start   | --embed-certs                  | minikube | deepa | v1.36.0 | 02 Sep 25 05:16 UTC | 02 Sep 25 05:16 UTC |
| start   | --embed-certs                  | minikube | deepa | v1.36.0 | 02 Sep 25 05:20 UTC | 02 Sep 25 05:20 UTC |
| start   | --memory=1800mb --embed-certs  | minikube | deepa | v1.36.0 | 02 Sep 25 05:20 UTC | 02 Sep 25 05:21 UTC |
| service | travel-api-service -n          | minikube | deepa | v1.36.0 | 02 Sep 25 05:36 UTC |                     |
|         | new-travel-api                 |          |       |         |                     |                     |
| service | travel-api-service -n          | minikube | deepa | v1.36.0 | 02 Sep 25 05:38 UTC |                     |
|         | new-travel-api                 |          |       |         |                     |                     |
| addons  | enable ingress                 | minikube | deepa | v1.36.0 | 02 Sep 25 05:44 UTC | 02 Sep 25 05:45 UTC |
| service | travel-api-service -n          | minikube | deepa | v1.36.0 | 02 Sep 25 05:45 UTC |                     |
|         | new-travel-api --url           |          |       |         |                     |                     |
| service | travel-api-service -n          | minikube | deepa | v1.36.0 | 02 Sep 25 05:53 UTC |                     |
|         | new-travel-api --url           |          |       |         |                     |                     |
|---------|--------------------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/09/02 05:20:50
Running on machine: Deepa
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0902 05:20:50.750074   53431 out.go:345] Setting OutFile to fd 1 ...
I0902 05:20:50.750948   53431 out.go:397] isatty.IsTerminal(1) = true
I0902 05:20:50.750952   53431 out.go:358] Setting ErrFile to fd 2...
I0902 05:20:50.750957   53431 out.go:397] isatty.IsTerminal(2) = true
I0902 05:20:50.751127   53431 root.go:338] Updating PATH: /home/deepa/.minikube/bin
W0902 05:20:50.751437   53431 root.go:314] Error reading config file at /home/deepa/.minikube/config/config.json: open /home/deepa/.minikube/config/config.json: no such file or directory
I0902 05:20:50.751776   53431 out.go:352] Setting JSON to false
I0902 05:20:50.756094   53431 start.go:130] hostinfo: {"hostname":"Deepa","uptime":4386,"bootTime":1756786065,"procs":101,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"f1066a02-f700-4591-9ac7-fb722f738586"}
I0902 05:20:50.756150   53431 start.go:140] virtualization: kvm guest
I0902 05:20:50.776274   53431 out.go:177] üòÑ  minikube v1.36.0 on Ubuntu 24.04 (kvm/amd64)
I0902 05:20:50.778633   53431 notify.go:220] Checking for updates...
I0902 05:20:50.778826   53431 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0902 05:20:50.779113   53431 driver.go:404] Setting default libvirt URI to qemu:///system
I0902 05:20:50.885270   53431 docker.go:123] docker version: linux-28.3.1:Docker Engine - Community
I0902 05:20:50.885490   53431 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0902 05:20:51.010362   53431 info.go:266] docker info: {ID:1d7a2b7d-dd12-47eb-896f-394a130284ff Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:30 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:36 OomKillDisable:false NGoroutines:54 SystemTime:2025-09-02 05:20:50.9881396 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:2969776128 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Deepa Labels:[] ExperimentalBuild:false ServerVersion:28.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.1]] Warnings:<nil>}}
I0902 05:20:51.010580   53431 docker.go:318] overlay module found
I0902 05:20:51.012732   53431 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0902 05:20:51.014609   53431 start.go:304] selected driver: docker
I0902 05:20:51.014650   53431 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:true MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/deepa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0902 05:20:51.014706   53431 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0902 05:20:51.018160   53431 out.go:201] 
W0902 05:20:51.019987   53431 out.go:270] ‚õî  Requested memory allocation (1800MB) is less than the recommended minimum 1900MB. Deployments may fail.
I0902 05:20:51.022035   53431 out.go:201] 
I0902 05:20:51.024387   53431 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0902 05:20:51.090911   53431 info.go:266] docker info: {ID:1d7a2b7d-dd12-47eb-896f-394a130284ff Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:30 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:36 OomKillDisable:false NGoroutines:54 SystemTime:2025-09-02 05:20:51.075610749 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:2969776128 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Deepa Labels:[] ExperimentalBuild:false ServerVersion:28.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.1]] Warnings:<nil>}}
I0902 05:20:51.093030   53431 out.go:201] 
W0902 05:20:51.094723   53431 out.go:270] ‚õî  Requested memory allocation (1800MB) is less than the recommended minimum 1900MB. Deployments may fail.
I0902 05:20:51.096320   53431 out.go:201] 
W0902 05:20:51.098143   53431 out.go:270] ‚ùó  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I0902 05:20:51.099794   53431 out.go:201] 
W0902 05:20:51.101492   53431 out.go:270] üßØ  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2832MiB). You may face stability issues.
W0902 05:20:51.101729   53431 out.go:270] üí°  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'
I0902 05:20:51.104600   53431 out.go:201] 
I0902 05:20:51.106366   53431 cni.go:84] Creating CNI manager for ""
I0902 05:20:51.106563   53431 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0902 05:20:51.106666   53431 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:true MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/deepa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0902 05:20:51.110210   53431 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0902 05:20:51.111873   53431 cache.go:121] Beginning downloading kic base image for docker with docker
I0902 05:20:51.113578   53431 out.go:177] üöú  Pulling base image v0.0.47 ...
I0902 05:20:51.115551   53431 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0902 05:20:51.115615   53431 preload.go:146] Found local preload: /home/deepa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0902 05:20:51.115621   53431 cache.go:56] Caching tarball of preloaded images
I0902 05:20:51.115650   53431 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0902 05:20:51.116018   53431 preload.go:172] Found /home/deepa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0902 05:20:51.116037   53431 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0902 05:20:51.116141   53431 profile.go:143] Saving config to /home/deepa/.minikube/profiles/minikube/config.json ...
I0902 05:20:51.160554   53431 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0902 05:20:51.160591   53431 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0902 05:20:51.160656   53431 cache.go:230] Successfully downloaded all kic artifacts
I0902 05:20:51.160809   53431 start.go:360] acquireMachinesLock for minikube: {Name:mk909908e46cd5c6edfdd5029bb9a9d98ce64332 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0902 05:20:51.160960   53431 start.go:364] duration metric: took 138.465¬µs to acquireMachinesLock for "minikube"
I0902 05:20:51.160997   53431 start.go:96] Skipping create...Using existing machine configuration
I0902 05:20:51.161017   53431 fix.go:54] fixHost starting: 
I0902 05:20:51.161217   53431 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 05:20:51.183731   53431 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0902 05:20:51.183747   53431 fix.go:138] unexpected machine state, will restart: <nil>
I0902 05:20:51.185922   53431 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0902 05:20:51.187711   53431 machine.go:93] provisionDockerMachine start ...
I0902 05:20:51.187890   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:51.210418   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:51.210702   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:51.210708   53431 main.go:141] libmachine: About to run SSH command:
hostname
I0902 05:20:51.355827   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0902 05:20:51.355922   53431 ubuntu.go:169] provisioning hostname "minikube"
I0902 05:20:51.356060   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:51.378702   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:51.378883   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:51.378887   53431 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0902 05:20:51.524667   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0902 05:20:51.524740   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:51.543977   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:51.544142   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:51.544159   53431 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0902 05:20:51.674867   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0902 05:20:51.674935   53431 ubuntu.go:175] set auth options {CertDir:/home/deepa/.minikube CaCertPath:/home/deepa/.minikube/certs/ca.pem CaPrivateKeyPath:/home/deepa/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/deepa/.minikube/machines/server.pem ServerKeyPath:/home/deepa/.minikube/machines/server-key.pem ClientKeyPath:/home/deepa/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/deepa/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/deepa/.minikube}
I0902 05:20:51.674965   53431 ubuntu.go:177] setting up certificates
I0902 05:20:51.674981   53431 provision.go:84] configureAuth start
I0902 05:20:51.675147   53431 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0902 05:20:51.704116   53431 provision.go:143] copyHostCerts
I0902 05:20:51.704263   53431 exec_runner.go:144] found /home/deepa/.minikube/ca.pem, removing ...
I0902 05:20:51.704306   53431 exec_runner.go:203] rm: /home/deepa/.minikube/ca.pem
I0902 05:20:51.704528   53431 exec_runner.go:151] cp: /home/deepa/.minikube/certs/ca.pem --> /home/deepa/.minikube/ca.pem (1074 bytes)
I0902 05:20:51.704920   53431 exec_runner.go:144] found /home/deepa/.minikube/cert.pem, removing ...
I0902 05:20:51.704923   53431 exec_runner.go:203] rm: /home/deepa/.minikube/cert.pem
I0902 05:20:51.704983   53431 exec_runner.go:151] cp: /home/deepa/.minikube/certs/cert.pem --> /home/deepa/.minikube/cert.pem (1119 bytes)
I0902 05:20:51.705111   53431 exec_runner.go:144] found /home/deepa/.minikube/key.pem, removing ...
I0902 05:20:51.705113   53431 exec_runner.go:203] rm: /home/deepa/.minikube/key.pem
I0902 05:20:51.705209   53431 exec_runner.go:151] cp: /home/deepa/.minikube/certs/key.pem --> /home/deepa/.minikube/key.pem (1675 bytes)
I0902 05:20:51.705302   53431 provision.go:117] generating server cert: /home/deepa/.minikube/machines/server.pem ca-key=/home/deepa/.minikube/certs/ca.pem private-key=/home/deepa/.minikube/certs/ca-key.pem org=deepa.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0902 05:20:51.858077   53431 provision.go:177] copyRemoteCerts
I0902 05:20:51.858242   53431 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0902 05:20:51.858299   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:51.879652   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:20:51.975284   53431 ssh_runner.go:362] scp /home/deepa/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0902 05:20:52.012500   53431 ssh_runner.go:362] scp /home/deepa/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0902 05:20:52.045197   53431 ssh_runner.go:362] scp /home/deepa/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0902 05:20:52.078115   53431 provision.go:87] duration metric: took 403.123866ms to configureAuth
I0902 05:20:52.078130   53431 ubuntu.go:193] setting minikube options for container-runtime
I0902 05:20:52.078264   53431 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0902 05:20:52.078303   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.099269   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:52.099451   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:52.099456   53431 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0902 05:20:52.229475   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0902 05:20:52.229486   53431 ubuntu.go:71] root file system type: overlay
I0902 05:20:52.229651   53431 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0902 05:20:52.229710   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.253474   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:52.253643   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:52.253689   53431 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0902 05:20:52.401224   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0902 05:20:52.401314   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.423766   53431 main.go:141] libmachine: Using SSH client type: native
I0902 05:20:52.423965   53431 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32788 <nil> <nil>}
I0902 05:20:52.423974   53431 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0902 05:20:52.560957   53431 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0902 05:20:52.560968   53431 machine.go:96] duration metric: took 1.37325135s to provisionDockerMachine
I0902 05:20:52.561034   53431 start.go:293] postStartSetup for "minikube" (driver="docker")
I0902 05:20:52.561042   53431 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0902 05:20:52.561090   53431 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0902 05:20:52.561125   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.582759   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:20:52.680530   53431 ssh_runner.go:195] Run: cat /etc/os-release
I0902 05:20:52.684839   53431 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0902 05:20:52.684851   53431 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0902 05:20:52.684856   53431 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0902 05:20:52.684861   53431 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0902 05:20:52.684892   53431 filesync.go:126] Scanning /home/deepa/.minikube/addons for local assets ...
I0902 05:20:52.684988   53431 filesync.go:126] Scanning /home/deepa/.minikube/files for local assets ...
I0902 05:20:52.685013   53431 start.go:296] duration metric: took 123.97401ms for postStartSetup
I0902 05:20:52.685155   53431 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0902 05:20:52.685197   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.705629   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:20:52.798895   53431 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0902 05:20:52.805431   53431 fix.go:56] duration metric: took 1.644417195s for fixHost
I0902 05:20:52.805442   53431 start.go:83] releasing machines lock for "minikube", held for 1.644475488s
I0902 05:20:52.805488   53431 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0902 05:20:52.826314   53431 ssh_runner.go:195] Run: cat /version.json
I0902 05:20:52.826356   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.826479   53431 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0902 05:20:52.826569   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:20:52.846330   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:20:52.846886   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:20:52.930578   53431 ssh_runner.go:195] Run: systemctl --version
I0902 05:20:53.106488   53431 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0902 05:20:53.114634   53431 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0902 05:20:53.151506   53431 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0902 05:20:53.151606   53431 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0902 05:20:53.170632   53431 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0902 05:20:53.170651   53431 start.go:495] detecting cgroup driver to use...
I0902 05:20:53.170710   53431 detect.go:190] detected "systemd" cgroup driver on host os
I0902 05:20:53.171206   53431 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0902 05:20:53.201608   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0902 05:20:53.215975   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0902 05:20:53.229894   53431 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0902 05:20:53.229962   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0902 05:20:53.244110   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0902 05:20:53.257701   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0902 05:20:53.271813   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0902 05:20:53.285575   53431 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0902 05:20:53.298027   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0902 05:20:53.310733   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0902 05:20:53.324173   53431 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0902 05:20:53.337891   53431 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0902 05:20:53.349222   53431 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0902 05:20:53.360821   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:20:53.502967   53431 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0902 05:21:04.846026   53431 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.41621126s)
I0902 05:21:04.846051   53431 start.go:495] detecting cgroup driver to use...
I0902 05:21:04.846085   53431 detect.go:190] detected "systemd" cgroup driver on host os
I0902 05:21:04.846275   53431 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0902 05:21:04.885773   53431 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0902 05:21:04.885819   53431 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0902 05:21:04.899838   53431 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0902 05:21:04.920894   53431 ssh_runner.go:195] Run: which cri-dockerd
I0902 05:21:04.925298   53431 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0902 05:21:04.936057   53431 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0902 05:21:04.961582   53431 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0902 05:21:05.040096   53431 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0902 05:21:05.136351   53431 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0902 05:21:05.136466   53431 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0902 05:21:05.160134   53431 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0902 05:21:05.176038   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:21:05.265935   53431 ssh_runner.go:195] Run: sudo systemctl restart docker
I0902 05:21:08.144050   53431 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.878071723s)
I0902 05:21:08.144126   53431 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0902 05:21:08.157509   53431 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0902 05:21:08.218687   53431 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0902 05:21:08.231680   53431 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0902 05:21:08.376134   53431 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0902 05:21:08.467835   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:21:08.538424   53431 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0902 05:21:08.588146   53431 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0902 05:21:08.603292   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:21:08.692583   53431 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0902 05:21:08.872565   53431 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0902 05:21:08.886748   53431 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0902 05:21:08.886853   53431 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0902 05:21:08.891058   53431 start.go:563] Will wait 60s for crictl version
I0902 05:21:08.891110   53431 ssh_runner.go:195] Run: which crictl
I0902 05:21:08.894892   53431 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0902 05:21:08.964914   53431 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0902 05:21:08.964980   53431 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0902 05:21:09.030874   53431 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0902 05:21:09.070444   53431 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0902 05:21:09.070687   53431 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0902 05:21:09.094124   53431 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0902 05:21:09.098573   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0902 05:21:09.120226   53431 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:true MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/deepa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0902 05:21:09.120407   53431 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0902 05:21:09.120455   53431 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0902 05:21:09.143363   53431 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0902 05:21:09.143372   53431 docker.go:632] Images already preloaded, skipping extraction
I0902 05:21:09.143472   53431 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0902 05:21:09.166724   53431 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0902 05:21:09.166820   53431 cache_images.go:84] Images are preloaded, skipping loading
I0902 05:21:09.166828   53431 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.33.1 docker true true} ...
I0902 05:21:09.167078   53431 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0902 05:21:09.167134   53431 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0902 05:21:09.276830   53431 cni.go:84] Creating CNI manager for ""
I0902 05:21:09.276856   53431 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0902 05:21:09.276899   53431 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0902 05:21:09.276915   53431 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0902 05:21:09.277051   53431 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0902 05:21:09.277164   53431 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0902 05:21:09.290354   53431 binaries.go:44] Found k8s binaries, skipping transfer
I0902 05:21:09.290411   53431 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0902 05:21:09.302626   53431 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0902 05:21:09.327907   53431 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0902 05:21:09.354381   53431 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0902 05:21:09.381127   53431 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0902 05:21:09.386927   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:21:09.596095   53431 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0902 05:21:09.661996   53431 certs.go:68] Setting up /home/deepa/.minikube/profiles/minikube for IP: 192.168.58.2
I0902 05:21:09.662069   53431 certs.go:194] generating shared ca certs ...
I0902 05:21:09.662086   53431 certs.go:226] acquiring lock for ca certs: {Name:mk83f0deb1cbb7fd578d7dc819871d2f2b19a20b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0902 05:21:09.662376   53431 certs.go:235] skipping valid "minikubeCA" ca cert: /home/deepa/.minikube/ca.key
I0902 05:21:09.662477   53431 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/deepa/.minikube/proxy-client-ca.key
I0902 05:21:09.662515   53431 certs.go:256] generating profile certs ...
I0902 05:21:09.662663   53431 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/deepa/.minikube/profiles/minikube/client.key
I0902 05:21:09.662868   53431 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/deepa/.minikube/profiles/minikube/apiserver.key.502bbb95
I0902 05:21:09.662963   53431 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/deepa/.minikube/profiles/minikube/proxy-client.key
I0902 05:21:09.663216   53431 certs.go:484] found cert: /home/deepa/.minikube/certs/ca-key.pem (1675 bytes)
I0902 05:21:09.663280   53431 certs.go:484] found cert: /home/deepa/.minikube/certs/ca.pem (1074 bytes)
I0902 05:21:09.663317   53431 certs.go:484] found cert: /home/deepa/.minikube/certs/cert.pem (1119 bytes)
I0902 05:21:09.663348   53431 certs.go:484] found cert: /home/deepa/.minikube/certs/key.pem (1675 bytes)
I0902 05:21:09.664987   53431 ssh_runner.go:362] scp /home/deepa/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0902 05:21:09.705841   53431 ssh_runner.go:362] scp /home/deepa/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0902 05:21:09.742614   53431 ssh_runner.go:362] scp /home/deepa/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0902 05:21:09.778470   53431 ssh_runner.go:362] scp /home/deepa/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0902 05:21:09.812517   53431 ssh_runner.go:362] scp /home/deepa/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0902 05:21:09.843725   53431 ssh_runner.go:362] scp /home/deepa/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0902 05:21:09.872864   53431 ssh_runner.go:362] scp /home/deepa/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0902 05:21:09.901825   53431 ssh_runner.go:362] scp /home/deepa/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0902 05:21:09.933655   53431 ssh_runner.go:362] scp /home/deepa/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0902 05:21:09.963807   53431 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0902 05:21:09.986599   53431 ssh_runner.go:195] Run: openssl version
I0902 05:21:09.994718   53431 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0902 05:21:10.008120   53431 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0902 05:21:10.012902   53431 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep  2 04:16 /usr/share/ca-certificates/minikubeCA.pem
I0902 05:21:10.012951   53431 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0902 05:21:10.021216   53431 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0902 05:21:10.033122   53431 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0902 05:21:10.037626   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0902 05:21:10.044307   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0902 05:21:10.051229   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0902 05:21:10.057867   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0902 05:21:10.064463   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0902 05:21:10.070728   53431 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0902 05:21:10.077400   53431 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:true MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/deepa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0902 05:21:10.077527   53431 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0902 05:21:10.101133   53431 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0902 05:21:10.113721   53431 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0902 05:21:10.113779   53431 kubeadm.go:589] restartPrimaryControlPlane start ...
I0902 05:21:10.113828   53431 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0902 05:21:10.125044   53431 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0902 05:21:10.125105   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0902 05:21:10.147760   53431 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32791"
I0902 05:21:10.150253   53431 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0902 05:21:10.161473   53431 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0902 05:21:10.161533   53431 kubeadm.go:593] duration metric: took 47.748487ms to restartPrimaryControlPlane
I0902 05:21:10.161539   53431 kubeadm.go:394] duration metric: took 84.174525ms to StartCluster
I0902 05:21:10.161578   53431 settings.go:142] acquiring lock: {Name:mk90d1114a42d9df76ed015931ecb2346107df24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0902 05:21:10.161724   53431 settings.go:150] Updating kubeconfig:  /home/deepa/.kube/config
I0902 05:21:10.162550   53431 lock.go:35] WriteFile acquiring /home/deepa/.kube/config: {Name:mk11d788829ca07330c37f11643a4e49959a3cfd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0902 05:21:10.162886   53431 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0902 05:21:10.163042   53431 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0902 05:21:10.163010   53431 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0902 05:21:10.163131   53431 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0902 05:21:10.163144   53431 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0902 05:21:10.163194   53431 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0902 05:21:10.163199   53431 addons.go:247] addon storage-provisioner should already be in state true
I0902 05:21:10.163241   53431 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0902 05:21:10.163267   53431 host.go:66] Checking if "minikube" exists ...
I0902 05:21:10.163516   53431 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 05:21:10.163654   53431 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 05:21:10.164934   53431 out.go:177] üîé  Verifying Kubernetes components...
I0902 05:21:10.166848   53431 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 05:21:10.185062   53431 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0902 05:21:10.185072   53431 addons.go:247] addon default-storageclass should already be in state true
I0902 05:21:10.185090   53431 host.go:66] Checking if "minikube" exists ...
I0902 05:21:10.185309   53431 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0902 05:21:10.185434   53431 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 05:21:10.187446   53431 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0902 05:21:10.187457   53431 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0902 05:21:10.187512   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:21:10.208980   53431 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0902 05:21:10.209005   53431 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0902 05:21:10.209125   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 05:21:10.209889   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:21:10.233936   53431 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32788 SSHKeyPath:/home/deepa/.minikube/machines/minikube/id_rsa Username:docker}
I0902 05:21:10.287069   53431 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0902 05:21:10.304260   53431 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0902 05:21:10.322029   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0902 05:21:10.327757   53431 api_server.go:52] waiting for apiserver process to appear ...
I0902 05:21:10.327818   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:10.342542   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:10.415549   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.415603   53431 retry.go:31] will retry after 221.847153ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0902 05:21:10.415640   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.415647   53431 retry.go:31] will retry after 323.365485ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.638127   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:10.695674   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.695691   53431 retry.go:31] will retry after 457.523233ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.739913   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:10.798779   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.798799   53431 retry.go:31] will retry after 218.004211ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:10.828926   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:11.017047   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:11.075311   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.075331   53431 retry.go:31] will retry after 281.310203ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.153473   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:11.208341   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.208357   53431 retry.go:31] will retry after 404.492943ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.328735   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:11.357041   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:11.411618   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.411633   53431 retry.go:31] will retry after 442.267387ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.613486   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:11.667981   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.667996   53431 retry.go:31] will retry after 980.735073ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.828586   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:11.854519   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:11.909489   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:11.909504   53431 retry.go:31] will retry after 1.001762102s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:12.328495   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:12.649889   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:12.716554   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:12.716576   53431 retry.go:31] will retry after 1.652159536s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:12.828763   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:12.911464   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:12.968378   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:12.968393   53431 retry.go:31] will retry after 2.200242745s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:13.329006   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:13.828615   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:14.328248   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:14.369221   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:14.436541   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:14.436609   53431 retry.go:31] will retry after 2.476799104s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:14.828495   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:15.169751   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:15.226982   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:15.227004   53431 retry.go:31] will retry after 3.396229978s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:15.328265   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:15.828310   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:16.328333   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:16.828434   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:16.914009   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:16.969430   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:16.969450   53431 retry.go:31] will retry after 3.963557077s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:17.329065   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:17.828882   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:18.328156   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:18.623710   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:18.689973   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:18.689989   53431 retry.go:31] will retry after 3.255112294s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:18.828310   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:19.328030   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:19.828099   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:20.328207   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:20.828973   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:20.934190   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:20.988961   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:20.988976   53431 retry.go:31] will retry after 6.065793206s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:21.328539   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:21.828792   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:21.946152   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0902 05:21:22.001221   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:22.001236   53431 retry.go:31] will retry after 8.830785423s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:22.329162   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:22.828251   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:23.327966   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:23.828857   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:24.328171   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:24.828546   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:25.328392   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:25.828327   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:26.328737   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:26.828461   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:27.055319   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0902 05:21:27.110911   53431 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:27.110927   53431 retry.go:31] will retry after 3.978042629s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0902 05:21:27.328468   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:27.828120   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:29.259378   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:29.759539   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:30.259136   53431 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 05:21:30.274044   53431 api_server.go:72] duration metric: took 19.180563862s to wait for apiserver process to appear ...
I0902 05:21:30.274055   53431 api_server.go:88] waiting for apiserver healthz status ...
I0902 05:21:30.274070   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:31.762982   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0902 05:21:32.020720   53431 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0902 05:21:35.276064   53431 api_server.go:269] stopped: https://127.0.0.1:32791/healthz: Get "https://127.0.0.1:32791/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0902 05:21:35.276185   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:36.589635   53431 api_server.go:279] https://127.0.0.1:32791/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0902 05:21:36.589669   53431 api_server.go:103] status: https://127.0.0.1:32791/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0902 05:21:36.589683   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:36.643506   53431 api_server.go:279] https://127.0.0.1:32791/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0902 05:21:36.643525   53431 api_server.go:103] status: https://127.0.0.1:32791/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0902 05:21:36.774810   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:36.782863   53431 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (5.019860726s)
I0902 05:21:36.783658   53431 api_server.go:279] https://127.0.0.1:32791/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 05:21:36.783707   53431 api_server.go:103] status: https://127.0.0.1:32791/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 05:21:37.184883   53431 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.164119077s)
I0902 05:21:37.187161   53431 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0902 05:21:37.190111   53431 addons.go:514] duration metric: took 26.09661151s for enable addons: enabled=[default-storageclass storage-provisioner]
I0902 05:21:37.274933   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:37.279254   53431 api_server.go:279] https://127.0.0.1:32791/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 05:21:37.279267   53431 api_server.go:103] status: https://127.0.0.1:32791/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 05:21:37.774859   53431 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32791/healthz ...
I0902 05:21:37.778090   53431 api_server.go:279] https://127.0.0.1:32791/healthz returned 200:
ok
I0902 05:21:37.779314   53431 api_server.go:141] control plane version: v1.33.1
I0902 05:21:37.779350   53431 api_server.go:131] duration metric: took 7.505290623s to wait for apiserver health ...
I0902 05:21:37.779488   53431 system_pods.go:43] waiting for kube-system pods to appear ...
I0902 05:21:37.785504   53431 system_pods.go:59] 8 kube-system pods found
I0902 05:21:37.785578   53431 system_pods.go:61] "coredns-674b8bbfcf-2gm4p" [7bfc3c58-a679-4f0b-854d-67085ba29c98] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0902 05:21:37.785584   53431 system_pods.go:61] "coredns-674b8bbfcf-qs5rv" [0ba344fc-c49d-45a1-9430-c200c23e1f46] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0902 05:21:37.785588   53431 system_pods.go:61] "etcd-minikube" [73572a05-83dc-4e22-bf2f-46f51a677f82] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0902 05:21:37.785613   53431 system_pods.go:61] "kube-apiserver-minikube" [37162740-544c-4dea-ae73-6e912115d9d5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0902 05:21:37.785628   53431 system_pods.go:61] "kube-controller-manager-minikube" [d47cb9ef-ec1d-4f77-8745-41f4d79a811f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0902 05:21:37.785631   53431 system_pods.go:61] "kube-proxy-lrjqk" [4dc72c15-da2d-4e45-bec5-3a1d5c3ce56f] Running
I0902 05:21:37.785634   53431 system_pods.go:61] "kube-scheduler-minikube" [a1912207-b162-490a-a140-0b6ffa80357e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0902 05:21:37.785636   53431 system_pods.go:61] "storage-provisioner" [894623a4-ee12-4be3-a8b1-9fdaeb91cf0b] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0902 05:21:37.785641   53431 system_pods.go:74] duration metric: took 6.105564ms to wait for pod list to return data ...
I0902 05:21:37.785671   53431 kubeadm.go:578] duration metric: took 26.692172504s to wait for: map[apiserver:true system_pods:true]
I0902 05:21:37.785702   53431 node_conditions.go:102] verifying NodePressure condition ...
I0902 05:21:37.788108   53431 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0902 05:21:37.788206   53431 node_conditions.go:123] node cpu capacity is 16
I0902 05:21:37.788395   53431 node_conditions.go:105] duration metric: took 2.690494ms to run NodePressure ...
I0902 05:21:37.788423   53431 start.go:241] waiting for startup goroutines ...
I0902 05:21:37.788427   53431 start.go:246] waiting for cluster config update ...
I0902 05:21:37.788472   53431 start.go:255] writing updated cluster config ...
I0902 05:21:37.790269   53431 ssh_runner.go:195] Run: rm -f paused
I0902 05:21:37.950544   53431 start.go:607] kubectl: 1.34.0, cluster: 1.33.1 (minor skew: 1)
I0902 05:21:37.952496   53431 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 02 05:21:06 minikube dockerd[11573]: time="2025-09-02T05:21:06.699547412Z" level=info msg="Loading containers: start."
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.040005910Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count edd0e1da1fc0f843f3525628c0aa8c7539177ea06872fd0f58758e63f07a1e10], retrying...."
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.101220098Z" level=info msg="Loading containers: done."
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.118136133Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.118220340Z" level=info msg="Initializing buildkit"
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.137878341Z" level=info msg="Completed buildkit initialization"
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.140985774Z" level=info msg="Daemon has completed initialization"
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.141077529Z" level=info msg="API listen on [::]:2376"
Sep 02 05:21:08 minikube dockerd[11573]: time="2025-09-02T05:21:08.141084597Z" level=info msg="API listen on /var/run/docker.sock"
Sep 02 05:21:08 minikube systemd[1]: Started Docker Application Container Engine.
Sep 02 05:21:08 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Sep 02 05:21:08 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Sep 02 05:21:08 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Sep 02 05:21:08 minikube systemd[1]: cri-docker.service: Consumed 1.371s CPU time.
Sep 02 05:21:08 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Start docker client with request timeout 0s"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Loaded network plugin cni"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Setting cgroupDriver systemd"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 02 05:21:08 minikube cri-dockerd[11920]: time="2025-09-02T05:21:08Z" level=info msg="Start cri-dockerd grpc backend"
Sep 02 05:21:08 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-2gm4p_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2fca5438d0637848bd938cb8a6069114073ae1dc8c906e3278d0583f4a55e967\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-2gm4p_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fa7e25043e94f2419814e63378f8f64d1d73392e1590814f3a289bd663d2fab7\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-2gm4p_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c714975fe2e3b18f03789e296e5ba6ce54d617a19d64b92df8a5cdbcf8a5abc2\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-qs5rv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0565de3da8eab3b26c70b88d210701f2cb7a050c706fb95ab737d3c30b93ceb3\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-qs5rv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8300c0ae2938510c2790d141beba14d83c52fa482415a1defb0f864e5820b61d\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-qs5rv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"614f4571f6a3af66db3f7a57c170d3a6880f28cb66e413ff9e803d94d45d8f0d\""
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3dbe51e9b12794385127eaf3a820353f58ebe5835d574cd5f9784155663c948d/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/70866bb9c8c5cf6d8db734b21f808b89b82ad340a94388586f6d2ed143f0be43/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6dbbe3c5c3dc5c853e696554ab20c0a7800b244c2535bdc53c74685e8bfeb2d4/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5cca8d5bb0cead9ae49fa56cc10df2417ab2adff08dc2987e97225a871dfa35e/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/988df9dd98ee1749330996054010e2095e16f47457eb37313142c0b8ac02b025/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f39c0186dc01d9ae20e800fabb68129070dcfe68d5208717e35176ab0fbd926/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:09 minikube cri-dockerd[11920]: time="2025-09-02T05:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/90e54d54961d3fe97cd660e5bb256f86843b0efe730e62c1d82ce11ef93f3561/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:12 minikube cri-dockerd[11920]: time="2025-09-02T05:21:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c614cca6c83d68a2b3525f26cc97bf0f602e3f36cbfc7b95669951ddb5cf431/resolv.conf as [nameserver 192.168.1.6 options ndots:0]"
Sep 02 05:21:20 minikube dockerd[11573]: time="2025-09-02T05:21:20.650722163Z" level=info msg="ignoring event" container=e2914f24829c50ad5aa4dac6e2f00e238c10e081b0c0ff61a2ce6c54958d5fdd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:21:25 minikube dockerd[11573]: time="2025-09-02T05:21:25.268777223Z" level=info msg="ignoring event" container=8a0e2b46bc02d30f7026976082a888e4471226851e9ef114750426b9e416cec2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:32:17 minikube cri-dockerd[11920]: time="2025-09-02T05:32:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/628f7f7e92f8cfb11e2d7c0d1e5d1aaf0d63664f8899f2b5549e53352570c851/resolv.conf as [nameserver 10.96.0.10 search new-travel-api.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 02 05:32:31 minikube cri-dockerd[11920]: time="2025-09-02T05:32:31Z" level=info msg="Pulling image container122333/apiimage:52: 90192eef3b0c: Downloading [===============================>                   ]  125.2MB/198.1MB"
Sep 02 05:32:41 minikube cri-dockerd[11920]: time="2025-09-02T05:32:41Z" level=info msg="Pulling image container122333/apiimage:52: db2e06b9ea92: Downloading [=================================================> ]  140.2MB/141.7MB"
Sep 02 05:32:43 minikube cri-dockerd[11920]: time="2025-09-02T05:32:43Z" level=info msg="Stop pulling image container122333/apiimage:52: Status: Downloaded newer image for container122333/apiimage:52"
Sep 02 05:44:42 minikube cri-dockerd[11920]: time="2025-09-02T05:44:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/71e88c220c9f9d08c7c52f97240c509bed3a8756e2118d04bf37c610d3b88289/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 02 05:44:42 minikube cri-dockerd[11920]: time="2025-09-02T05:44:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/957542b427118f4bfcfbecab31bc4b10543a049af97f41e7fee2f2bd7513e0e6/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 02 05:44:42 minikube dockerd[11573]: time="2025-09-02T05:44:42.615405246Z" level=warning msg="reference for unknown type: " digest="sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Sep 02 05:44:46 minikube cri-dockerd[11920]: time="2025-09-02T05:44:46Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Sep 02 05:44:46 minikube cri-dockerd[11920]: time="2025-09-02T05:44:46Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Sep 02 05:44:46 minikube dockerd[11573]: time="2025-09-02T05:44:46.820775401Z" level=info msg="ignoring event" container=fbeb27375fe624409b30f41792be65805d60cef3a1686a439d044457b6b6db19 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:44:46 minikube dockerd[11573]: time="2025-09-02T05:44:46.874961657Z" level=info msg="ignoring event" container=84afc1df1c8ca0e05a430a98405552824c496edcaa8997e68267b1730fd29d0d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:44:47 minikube dockerd[11573]: time="2025-09-02T05:44:47.629063380Z" level=info msg="ignoring event" container=b382a18c5833e4ac95da0986e0e84a319c53fb5c205e5e30cd2164850d8aa815 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:44:48 minikube dockerd[11573]: time="2025-09-02T05:44:48.794169068Z" level=info msg="ignoring event" container=957542b427118f4bfcfbecab31bc4b10543a049af97f41e7fee2f2bd7513e0e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:44:49 minikube cri-dockerd[11920]: time="2025-09-02T05:44:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2883c667f131613a9f23dc21477eec22948783959d7787a829d41041d112a7c7/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 02 05:44:49 minikube dockerd[11573]: time="2025-09-02T05:44:49.887513701Z" level=info msg="ignoring event" container=71e88c220c9f9d08c7c52f97240c509bed3a8756e2118d04bf37c610d3b88289 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 05:44:50 minikube dockerd[11573]: time="2025-09-02T05:44:50.024615138Z" level=warning msg="reference for unknown type: " digest="sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9" remote="registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"
Sep 02 05:45:00 minikube cri-dockerd[11920]: time="2025-09-02T05:45:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: f5661824f928: Downloading [==========================================>        ]  18.43MB/21.6MB"
Sep 02 05:45:01 minikube cri-dockerd[11920]: time="2025-09-02T05:45:01Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.12.2@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
5b0a648b096e1       registry.k8s.io/ingress-nginx/controller@sha256:03497ee984628e95eca9b2279e3f3a3c1685dd48635479e627d219f00c8eefa9             11 minutes ago      Running             controller                0                   2883c667f1316       ingress-nginx-controller-67c5cb88f-xnm5d
b382a18c5833e       fcb7220db7030                                                                                                                11 minutes ago      Exited              patch                     1                   71e88c220c9f9       ingress-nginx-admission-patch-j6j79
84afc1df1c8ca       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524   11 minutes ago      Exited              create                    0                   957542b427118       ingress-nginx-admission-create-tmcnp
1d5ed550a4762       container122333/apiimage@sha256:dc21daa578f632dc8bc848a6f052db5a609c9b78829cf457c7eac8a908fced29                             23 minutes ago      Running             travelapi                 0                   628f7f7e92f8c       travel-api-deployment-ccd8bf76-sc866
06cca6cc885db       6e38f40d628db                                                                                                                33 minutes ago      Running             storage-provisioner       4                   5cca8d5bb0cea       storage-provisioner
d4d068ce19a8d       ef43894fa110c                                                                                                                34 minutes ago      Running             kube-controller-manager   4                   90e54d54961d3       kube-controller-manager-minikube
34f29b0e48e28       499038711c081                                                                                                                34 minutes ago      Running             etcd                      3                   70866bb9c8c5c       etcd-minikube
9c1a20e0fe33b       c6ab243b29f82                                                                                                                34 minutes ago      Running             kube-apiserver            3                   4c614cca6c83d       kube-apiserver-minikube
8a0e2b46bc02d       6e38f40d628db                                                                                                                34 minutes ago      Exited              storage-provisioner       3                   5cca8d5bb0cea       storage-provisioner
1cf499d9540de       1cf5f116067c6                                                                                                                34 minutes ago      Running             coredns                   3                   988df9dd98ee1       coredns-674b8bbfcf-2gm4p
9f7779794762b       b79c189b052cd                                                                                                                34 minutes ago      Running             kube-proxy                3                   3dbe51e9b1279       kube-proxy-lrjqk
0a1828bf4e2fc       1cf5f116067c6                                                                                                                34 minutes ago      Running             coredns                   3                   7f39c0186dc01       coredns-674b8bbfcf-qs5rv
0af4f961be4e2       398c985c0d950                                                                                                                34 minutes ago      Running             kube-scheduler            3                   6dbbe3c5c3dc5       kube-scheduler-minikube
e2914f24829c5       ef43894fa110c                                                                                                                34 minutes ago      Exited              kube-controller-manager   3                   90e54d54961d3       kube-controller-manager-minikube
392e4dea16ce1       c6ab243b29f82                                                                                                                35 minutes ago      Exited              kube-apiserver            2                   649ae0ad86746       kube-apiserver-minikube
418ade6445731       499038711c081                                                                                                                35 minutes ago      Exited              etcd                      2                   5954d3c993e77       etcd-minikube
7ab2e09017439       398c985c0d950                                                                                                                35 minutes ago      Exited              kube-scheduler            2                   708ac11259e55       kube-scheduler-minikube
f2cc811d28e9c       1cf5f116067c6                                                                                                                35 minutes ago      Exited              coredns                   2                   2fca5438d0637       coredns-674b8bbfcf-2gm4p
b8e0c220da5a2       1cf5f116067c6                                                                                                                35 minutes ago      Exited              coredns                   2                   0565de3da8eab       coredns-674b8bbfcf-qs5rv
70924cd8ebb86       b79c189b052cd                                                                                                                35 minutes ago      Exited              kube-proxy                2                   5ec63eb50a36d       kube-proxy-lrjqk


==> controller_ingress [5b0a648b096e] <==
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 29#29: sendmsg() failed (9: Bad file descriptor)
2025/09/02 05:55:37 [alert] 550#550: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 553#553: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 556#556: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 555#555: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 554#554: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 561#561: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 552#552: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 557#557: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 558#558: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 560#560: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 559#559: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 568#568: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:37 [alert] 29#29: worker process 549 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 550 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 551 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 552 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 553 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 554 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 555 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 556 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 557 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 558 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 559 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 560 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 561 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:37 [alert] 29#29: worker process 568 exited with fatal code 2 and cannot be respawned
W0902 05:55:53.614800       7 controller.go:1219] Service "new-travel-api/travel-api-service" does not have any active Endpoint.
W0902 05:55:53.614962       7 controller.go:1441] Error getting SSL certificate "new-travel-api/travel-api-tls": local SSL certificate new-travel-api/travel-api-tls was not found. Using default certificate
I0902 05:55:53.615082       7 controller.go:196] "Configuration changes detected, backend reload required"
I0902 05:55:53.663088       7 controller.go:216] "Backend successfully reloaded"
I0902 05:55:53.663393       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-67c5cb88f-xnm5d", UID:"54f4e77b-bb62-4890-bd51-a60cad1d3919", APIVersion:"v1", ResourceVersion:"2130", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
2025/09/02 05:55:53 [alert] 631#631: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:53 [alert] 629#629: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:53 [alert] 632#632: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:53 [alert] 625#625: pthread_create() failed (11: Resource temporarily unavailable)
2025/09/02 05:55:53 [alert] 29#29: worker process 625 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:53 [alert] 29#29: worker process 631 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:53 [alert] 29#29: worker process 632 exited with fatal code 2 and cannot be respawned
2025/09/02 05:55:53 [alert] 29#29: worker process 629 exited with fatal code 2 and cannot be respawned


==> coredns [0a1828bf4e2f] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: services is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "services" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: namespaces is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "namespaces" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpointslices" in API group "discovery.k8s.io" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [1cf499d9540d] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [b8e0c220da5a] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [f2cc811d28e9] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_02T05_14_13_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 02 Sep 2025 05:14:10 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 02 Sep 2025 05:56:00 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 02 Sep 2025 05:55:37 +0000   Tue, 02 Sep 2025 05:14:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 02 Sep 2025 05:55:37 +0000   Tue, 02 Sep 2025 05:14:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 02 Sep 2025 05:55:37 +0000   Tue, 02 Sep 2025 05:14:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 02 Sep 2025 05:55:37 +0000   Tue, 02 Sep 2025 05:14:10 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2900172Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2900172Ki
  pods:               110
System Info:
  Machine ID:                 67dcb2aa3107451e99fd9501f4b682af
  System UUID:                67dcb2aa3107451e99fd9501f4b682af
  Boot ID:                    ca358d0b-3ecd-4037-9eff-c4d83cea54f6
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-67c5cb88f-xnm5d    100m (0%)     0 (0%)      90Mi (3%)        0 (0%)         11m
  kube-system                 coredns-674b8bbfcf-2gm4p                    100m (0%)     0 (0%)      70Mi (2%)        170Mi (6%)     41m
  kube-system                 coredns-674b8bbfcf-qs5rv                    100m (0%)     0 (0%)      70Mi (2%)        170Mi (6%)     41m
  kube-system                 etcd-minikube                               100m (0%)     0 (0%)      100Mi (3%)       0 (0%)         41m
  kube-system                 kube-apiserver-minikube                     250m (1%)     0 (0%)      0 (0%)           0 (0%)         41m
  kube-system                 kube-controller-manager-minikube            200m (1%)     0 (0%)      0 (0%)           0 (0%)         41m
  kube-system                 kube-proxy-lrjqk                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m
  kube-system                 kube-scheduler-minikube                     100m (0%)     0 (0%)      0 (0%)           0 (0%)         41m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m
  new-travel-api              travel-api-deployment-ccd8bf76-sc866        0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (5%)    0 (0%)
  memory             330Mi (11%)  340Mi (12%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 41m                kube-proxy       
  Normal   Starting                 34m                kube-proxy       
  Normal   Starting                 35m                kube-proxy       
  Normal   Starting                 39m                kube-proxy       
  Normal   NodeHasNoDiskPressure    41m (x8 over 41m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     41m (x7 over 41m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  41m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  41m (x8 over 41m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                 41m                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  41m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  41m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    41m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     41m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           41m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           39m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed        35m                kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   RegisteredNode           34m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
               mapped:48076 shmem:666 pagetables:2454
               sec_pagetables:0 bounce:0
               kernel_misc_reclaimable:0
               free:19802 free_pcp:810 free_cma:0
[  +0.000954] Node 0 active_anon:320856kB inactive_anon:462984kB active_file:374516kB inactive_file:1236792kB unevictable:0kB isolated(anon):0kB isolated(file):0kB mapped:192304kB dirty:145744kB writeback:0kB shmem:2664kB shmem_thp:0kB shmem_pmdmapped:0kB anon_thp:0kB writeback_tmp:0kB kernel_stack:8832kB pagetables:9816kB sec_pagetables:0kB all_unreclaimable? no
[  +0.000070] Node 0 DMA32 free:79208kB boost:0kB min:22528kB low:28160kB high:33792kB reserved_highatomic:30720KB active_anon:320856kB inactive_anon:463096kB active_file:374516kB inactive_file:1237040kB unevictable:0kB writepending:145500kB present:3068540kB managed:2900172kB mlocked:0kB bounce:0kB free_pcp:3456kB local_pcp:0kB free_cma:0kB
[  +0.000008] lowmem_reserve[]: 0 0 0 0
[  +0.000007] Node 0 DMA32: 4086*4kB (UMH) 1304*8kB (UMEH) 553*16kB (UMEH) 406*32kB (UMEH) 144*64kB (UMEH) 82*128kB (UMEH) 19*256kB (UEH) 5*512kB (H) 3*1024kB (H) 0*2048kB 0*4096kB = 78824kB
[  +0.000420] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB
[  +0.000069] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB
[  +0.000002] 408723 total pagecache pages
[  +0.000002] 5096 pages in swap cache
[  +0.000001] Free swap  = 348876kB
[  +0.000001] Total swap = 1048576kB
[  +0.000001] 767135 pages RAM
[  +0.000001] 0 pages HighMem/MovableOnly
[  +0.000001] 42092 pages reserved
[  +0.000001] 0 pages hwpoisoned
[Sep 2 05:15] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:16] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:17] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:19] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:20] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:21] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:22] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:23] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:24] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:25] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:26] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:27] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:28] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:29] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:30] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:31] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:32] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:33] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:34] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:35] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:36] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:37] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:38] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:39] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:40] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:41] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:42] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:43] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:44] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:45] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:46] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:47] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:48] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:49] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:50] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:51] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:52] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:53] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:54] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:55] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:57] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)
[Sep 2 05:58] WSL (36578 - Relay) ERROR: UtilAcceptVsock:244: Waiting for abnormally long accept(11)


==> etcd [34f29b0e48e2] <==
{"level":"info","ts":"2025-09-02T05:21:34.207588Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2025-09-02T05:21:34.207798Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-09-02T05:21:34.215256Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"7.073272ms"}
{"level":"info","ts":"2025-09-02T05:21:34.223589Z","caller":"etcdserver/server.go:534","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-09-02T05:21:34.228393Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":962}
{"level":"info","ts":"2025-09-02T05:21:34.228631Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=()"}
{"level":"info","ts":"2025-09-02T05:21:34.228807Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 4"}
{"level":"info","ts":"2025-09-02T05:21:34.228855Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [], term: 4, commit: 962, applied: 0, lastindex: 962, lastterm: 4]"}
{"level":"warn","ts":"2025-09-02T05:21:34.230254Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-09-02T05:21:34.233795Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":869}
{"level":"info","ts":"2025-09-02T05:21:34.233894Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":962}
{"level":"info","ts":"2025-09-02T05:21:34.235817Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-09-02T05:21:34.237869Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2025-09-02T05:21:34.238224Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2025-09-02T05:21:34.238275Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-09-02T05:21:34.238407Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-09-02T05:21:34.238767Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:21:34.238929Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:21:34.238940Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:21:34.238822Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:21:34.239322Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2025-09-02T05:21:34.239461Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","added-peer-id":"b2c6679ac05f2cf1","added-peer-peer-urls":["https://192.168.58.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-09-02T05:21:34.239699Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","cluster-version":"3.5"}
{"level":"info","ts":"2025-09-02T05:21:34.239761Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-09-02T05:21:34.240619Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-09-02T05:21:34.240857Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:21:34.240968Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:21:34.240975Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-09-02T05:21:34.241006Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-09-02T05:21:35.229823Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 4"}
{"level":"info","ts":"2025-09-02T05:21:35.229902Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 4"}
{"level":"info","ts":"2025-09-02T05:21:35.229928Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 4"}
{"level":"info","ts":"2025-09-02T05:21:35.229941Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 5"}
{"level":"info","ts":"2025-09-02T05:21:35.230023Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 5"}
{"level":"info","ts":"2025-09-02T05:21:35.230045Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 5"}
{"level":"info","ts":"2025-09-02T05:21:35.230053Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 5"}
{"level":"info","ts":"2025-09-02T05:21:35.246292Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2025-09-02T05:21:35.246307Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-09-02T05:21:35.246362Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-09-02T05:21:35.246731Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-09-02T05:21:35.246845Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-09-02T05:21:35.247786Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:21:35.247963Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:21:35.248408Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"info","ts":"2025-09-02T05:21:35.248444Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-09-02T05:31:51.321367Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1197}
{"level":"info","ts":"2025-09-02T05:31:51.374673Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1197,"took":"50.015148ms","hash":4275114065,"current-db-size-bytes":2895872,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-09-02T05:31:51.374828Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4275114065,"revision":1197,"compact-revision":-1}
{"level":"info","ts":"2025-09-02T05:36:59.820446Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1439}
{"level":"info","ts":"2025-09-02T05:36:59.875826Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1439,"took":"52.825714ms","hash":932341112,"current-db-size-bytes":2895872,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1642496,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-09-02T05:36:59.876166Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":932341112,"revision":1439,"compact-revision":1197}
{"level":"info","ts":"2025-09-02T05:42:08.389082Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1707}
{"level":"info","ts":"2025-09-02T05:42:08.426767Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1707,"took":"20.87391ms","hash":4124112879,"current-db-size-bytes":2895872,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1728512,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-09-02T05:42:08.426874Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4124112879,"revision":1707,"compact-revision":1439}
{"level":"info","ts":"2025-09-02T05:47:17.192450Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1954}
{"level":"info","ts":"2025-09-02T05:47:17.231733Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1954,"took":"37.184704ms","hash":3462720668,"current-db-size-bytes":2895872,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":2084864,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-09-02T05:47:17.232173Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3462720668,"revision":1954,"compact-revision":1707}
{"level":"info","ts":"2025-09-02T05:52:25.505684Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2325}
{"level":"info","ts":"2025-09-02T05:52:25.533861Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2325,"took":"26.706487ms","hash":1443816964,"current-db-size-bytes":2895872,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":2215936,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-09-02T05:52:25.533947Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1443816964,"revision":2325,"compact-revision":1954}


==> etcd [418ade644573] <==
{"level":"info","ts":"2025-09-02T05:20:38.128055Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.58.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.58.2:2380","--initial-cluster=minikube=https://192.168.58.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.58.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.58.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-09-02T05:20:38.128166Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-09-02T05:20:38.128222Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-09-02T05:20:38.128242Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2025-09-02T05:20:38.128297Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-09-02T05:20:38.129481Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2025-09-02T05:20:38.130366Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-09-02T05:20:38.147057Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"16.209226ms"}
{"level":"info","ts":"2025-09-02T05:20:38.157908Z","caller":"etcdserver/server.go:534","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-09-02T05:20:38.164890Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":850}
{"level":"info","ts":"2025-09-02T05:20:38.166162Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=()"}
{"level":"info","ts":"2025-09-02T05:20:38.166342Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 3"}
{"level":"info","ts":"2025-09-02T05:20:38.166373Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [], term: 3, commit: 850, applied: 0, lastindex: 850, lastterm: 3]"}
{"level":"warn","ts":"2025-09-02T05:20:38.167907Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-09-02T05:20:38.172935Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":763}
{"level":"info","ts":"2025-09-02T05:20:38.173089Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":850}
{"level":"info","ts":"2025-09-02T05:20:38.175006Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-09-02T05:20:38.177819Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2025-09-02T05:20:38.178325Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2025-09-02T05:20:38.178358Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-09-02T05:20:38.178558Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-09-02T05:20:38.179084Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:20:38.179110Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:20:38.179328Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:20:38.179343Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2025-09-02T05:20:38.179361Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-09-02T05:20:38.179459Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","added-peer-id":"b2c6679ac05f2cf1","added-peer-peer-urls":["https://192.168.58.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-09-02T05:20:38.180951Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","cluster-version":"3.5"}
{"level":"info","ts":"2025-09-02T05:20:38.181013Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-09-02T05:20:38.182911Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-09-02T05:20:38.183975Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:20:38.184041Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:20:38.184199Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-09-02T05:20:38.184228Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-09-02T05:20:40.067930Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 3"}
{"level":"info","ts":"2025-09-02T05:20:40.068011Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 3"}
{"level":"info","ts":"2025-09-02T05:20:40.068063Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 3"}
{"level":"info","ts":"2025-09-02T05:20:40.068077Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 4"}
{"level":"info","ts":"2025-09-02T05:20:40.068177Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 4"}
{"level":"info","ts":"2025-09-02T05:20:40.068191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 4"}
{"level":"info","ts":"2025-09-02T05:20:40.068200Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 4"}
{"level":"info","ts":"2025-09-02T05:20:40.072826Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2025-09-02T05:20:40.072837Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-09-02T05:20:40.072849Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-09-02T05:20:40.073189Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-09-02T05:20:40.073271Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-09-02T05:20:40.074485Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:20:40.074618Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-09-02T05:20:40.074995Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-09-02T05:20:40.075090Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"info","ts":"2025-09-02T05:20:53.546310Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-09-02T05:20:53.548919Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"info","ts":"2025-09-02T05:21:01.477984Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b2c6679ac05f2cf1","current-leader-member-id":"b2c6679ac05f2cf1"}
{"level":"warn","ts":"2025-09-02T05:21:01.478387Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-09-02T05:21:01.478599Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-09-02T05:21:01.479107Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-09-02T05:21:01.479157Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2025-09-02T05:21:01.484493Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:21:01.484820Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-09-02T05:21:01.484844Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}


==> kernel <==
 05:56:06 up  1:47,  0 users,  load average: 0.24, 0.16, 0.19
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [392e4dea16ce] <==
W0902 05:21:00.036252       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.092387       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.118337       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.162071       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.271281       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.287414       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.290952       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:00.412568       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:02.456173       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:02.486681       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:02.762047       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:02.862858       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:02.984934       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.119425       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.184027       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.185812       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.197607       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.272755       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.300564       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.332065       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.418009       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.420493       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.435087       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.492330       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.589788       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.609927       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.614399       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.668786       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.715360       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.724889       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.799833       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.818436       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.830008       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.849909       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.849990       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.861660       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.873240       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.890174       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.896324       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.926608       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.926628       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.947415       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.961825       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.981419       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:03.991783       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.016375       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.057819       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.129222       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.167643       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.182142       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.193065       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.204823       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.221303       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.234700       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.270718       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.277338       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.353458       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.425308       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.433690       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0902 05:21:04.440079       1 logging.go:55] [core] [Channel #19 SubChannel #20]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [9c1a20e0fe33] <==
I0902 05:21:36.570202       1 controller.go:142] Starting OpenAPI controller
I0902 05:21:36.570243       1 controller.go:90] Starting OpenAPI V3 controller
I0902 05:21:36.570253       1 naming_controller.go:299] Starting NamingConditionController
I0902 05:21:36.570260       1 establishing_controller.go:81] Starting EstablishingController
I0902 05:21:36.570301       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0902 05:21:36.570529       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0902 05:21:36.570548       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0902 05:21:36.570917       1 controller.go:78] Starting OpenAPI AggregationController
I0902 05:21:36.571057       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0902 05:21:36.571276       1 local_available_controller.go:156] Starting LocalAvailability controller
I0902 05:21:36.571284       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0902 05:21:36.572047       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0902 05:21:36.572106       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0902 05:21:36.586059       1 repairip.go:200] Starting ipallocator-repair-controller
I0902 05:21:36.586143       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0902 05:21:36.670673       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0902 05:21:36.670771       1 aggregator.go:171] initial CRD sync complete...
I0902 05:21:36.670777       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0902 05:21:36.670823       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0902 05:21:36.670901       1 autoregister_controller.go:144] Starting autoregister controller
I0902 05:21:36.670927       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0902 05:21:36.670934       1 cache.go:39] Caches are synced for autoregister controller
I0902 05:21:36.670968       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0902 05:21:36.671001       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0902 05:21:36.671008       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0902 05:21:36.671030       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0902 05:21:36.671402       1 cache.go:39] Caches are synced for LocalAvailability controller
I0902 05:21:36.673163       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0902 05:21:36.673186       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0902 05:21:36.687069       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0902 05:21:36.709184       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0902 05:21:36.711881       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0902 05:21:36.718060       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0902 05:21:36.718202       1 policy_source.go:240] refreshing policies
I0902 05:21:36.722405       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:21:36.730967       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0902 05:21:37.572947       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0902 05:22:03.805114       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0902 05:22:05.204491       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:22:05.299187       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0902 05:22:05.398758       1 controller.go:667] quota admission added evaluator for: endpoints
I0902 05:30:52.776359       1 controller.go:667] quota admission added evaluator for: namespaces
I0902 05:31:52.144124       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:32:17.205817       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0902 05:32:17.236151       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0902 05:35:01.025313       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:38:05.061016       1 alloc.go:328] "allocated clusterIPs" service="new-travel-api/travel-api-service" clusterIPs={"IPv4":"10.101.76.215"}
I0902 05:38:05.067201       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:39:18.933109       1 controller.go:667] quota admission added evaluator for: ingresses.networking.k8s.io
I0902 05:42:09.172884       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:44:41.287650       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0902 05:44:41.316901       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0902 05:44:41.384837       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.98.28.138"}
I0902 05:44:41.388335       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:44:41.414590       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.103.245.170"}
I0902 05:44:41.450449       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0902 05:52:26.243534       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:55:37.389207       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0902 05:55:53.608099       1 alloc.go:328] "allocated clusterIPs" service="new-travel-api/travel-api-service" clusterIPs={"IPv4":"10.103.251.229"}
I0902 05:55:53.611103       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [d4d068ce19a8] <==
I0902 05:22:04.896772       1 shared_informer.go:350] "Waiting for caches to sync" controller="disruption"
I0902 05:22:04.946388       1 controllermanager.go:778] "Started controller" controller="ttl-controller"
I0902 05:22:04.946526       1 ttl_controller.go:127] "Starting TTL controller" logger="ttl-controller"
I0902 05:22:04.946549       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL"
I0902 05:22:04.996288       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0902 05:22:04.996558       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0902 05:22:04.996601       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0902 05:22:04.999585       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0902 05:22:05.003953       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0902 05:22:05.011028       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0902 05:22:05.012856       1 shared_informer.go:357] "Caches are synced" controller="node"
I0902 05:22:05.012939       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0902 05:22:05.013032       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0902 05:22:05.013050       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0902 05:22:05.013055       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0902 05:22:05.014744       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0902 05:22:05.024281       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0902 05:22:05.025542       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0902 05:22:05.028923       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0902 05:22:05.034604       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0902 05:22:05.036905       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0902 05:22:05.047452       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0902 05:22:05.047491       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0902 05:22:05.047813       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0902 05:22:05.048200       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0902 05:22:05.048367       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0902 05:22:05.048330       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0902 05:22:05.054602       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0902 05:22:05.055836       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0902 05:22:05.068538       1 shared_informer.go:357] "Caches are synced" controller="job"
I0902 05:22:05.070883       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0902 05:22:05.070907       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0902 05:22:05.072106       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0902 05:22:05.074319       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0902 05:22:05.080625       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0902 05:22:05.095964       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0902 05:22:05.097202       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0902 05:22:05.097233       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0902 05:22:05.098502       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0902 05:22:05.099280       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0902 05:22:05.127828       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0902 05:22:05.135648       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0902 05:22:05.147645       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0902 05:22:05.160380       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0902 05:22:05.163598       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0902 05:22:05.179912       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0902 05:22:05.230355       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0902 05:22:05.232674       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0902 05:22:05.246176       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0902 05:22:05.296788       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0902 05:22:05.297536       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0902 05:22:05.396845       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0902 05:22:05.399177       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0902 05:22:05.399640       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0902 05:22:05.446487       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0902 05:22:05.450061       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0902 05:22:05.811912       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0902 05:22:05.832779       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0902 05:22:05.832810       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0902 05:22:05.832817       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-controller-manager [e2914f24829c] <==
I0902 05:21:10.275144       1 serving.go:386] Generated self-signed cert in-memory
I0902 05:21:10.602504       1 controllermanager.go:188] "Starting" version="v1.33.1"
I0902 05:21:10.602537       1 controllermanager.go:190] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0902 05:21:10.604880       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0902 05:21:10.605003       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0902 05:21:10.604884       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0902 05:21:10.605092       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0902 05:21:20.607399       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://192.168.58.2:8443/healthz\": dial tcp 192.168.58.2:8443: connect: connection refused"


==> kube-proxy [70924cd8ebb8] <==
I0902 05:20:32.547654       1 server_linux.go:63] "Using iptables proxy"
E0902 05:20:32.774019       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:20:33.849677       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:20:36.022804       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:20:40.390353       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
I0902 05:20:49.770733       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0902 05:20:49.771899       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0902 05:20:49.812396       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0902 05:20:49.812796       1 server_linux.go:145] "Using iptables Proxier"
I0902 05:20:49.821692       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0902 05:20:49.824227       1 server.go:516] "Version info" version="v1.33.1"
I0902 05:20:49.824292       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0902 05:20:49.837934       1 config.go:199] "Starting service config controller"
I0902 05:20:49.838148       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0902 05:20:49.838299       1 config.go:105] "Starting endpoint slice config controller"
I0902 05:20:49.838328       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0902 05:20:49.838751       1 config.go:440] "Starting serviceCIDR config controller"
I0902 05:20:49.838770       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0902 05:20:49.838795       1 config.go:329] "Starting node config controller"
I0902 05:20:49.838822       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0902 05:20:49.939408       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0902 05:20:49.939408       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0902 05:20:49.939441       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0902 05:20:49.939452       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [9f7779794762] <==
I0902 05:21:23.324399       1 server_linux.go:63] "Using iptables proxy"
E0902 05:21:23.480413       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:21:24.550610       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:21:26.853868       1 server.go:704] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
E0902 05:21:36.633233       1 server.go:704] "Failed to retrieve node info" err="nodes \"minikube\" is forbidden: User \"system:serviceaccount:kube-system:kube-proxy\" cannot get resource \"nodes\" in API group \"\" at the cluster scope"
I0902 05:21:44.976268       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0902 05:21:44.976677       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0902 05:21:45.022082       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0902 05:21:45.022216       1 server_linux.go:145] "Using iptables Proxier"
I0902 05:21:45.029475       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0902 05:21:45.030549       1 server.go:516] "Version info" version="v1.33.1"
I0902 05:21:45.030584       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0902 05:21:45.033724       1 config.go:199] "Starting service config controller"
I0902 05:21:45.033747       1 config.go:105] "Starting endpoint slice config controller"
I0902 05:21:45.033886       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0902 05:21:45.033983       1 config.go:329] "Starting node config controller"
I0902 05:21:45.034008       1 config.go:440] "Starting serviceCIDR config controller"
I0902 05:21:45.034022       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0902 05:21:45.034024       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0902 05:21:45.033884       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0902 05:21:45.134300       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0902 05:21:45.134335       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0902 05:21:45.134367       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0902 05:21:45.134382       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [0af4f961be4e] <==
E0902 05:21:15.469745       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:21:15.479565       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:21:15.627588       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:21:15.653429       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:21:15.660463       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:21:15.688523       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:21:15.702549       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:21:15.795272       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:21:15.854534       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:21:15.861346       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:21:15.901407       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:21:16.005289       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:21:17.443025       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:21:17.590554       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:21:17.775094       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:21:17.890655       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:21:18.148563       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:21:18.151531       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:21:18.207588       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:21:18.300924       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:21:18.384133       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:21:18.525060       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0902 05:21:18.539131       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:21:18.631350       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:21:18.644249       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:21:18.697398       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:21:18.826354       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:21:18.911699       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:21:21.489819       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:21:21.727936       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:21:21.755992       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:21:22.206453       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:21:22.453424       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:21:22.585050       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:21:22.654592       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:21:22.659547       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0902 05:21:23.692089       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:21:23.723176       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:21:24.157629       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:21:24.157632       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:21:24.214602       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:21:24.281443       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:21:24.342461       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:21:24.612734       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:21:29.443893       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:21:36.583706       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:21:36.583706       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:21:36.583706       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:21:36.583812       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:21:36.583884       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:21:36.583890       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:21:36.583948       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:21:36.583963       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:21:36.583706       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:21:36.607152       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:21:36.607152       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:21:36.607194       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:21:36.623434       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:21:36.625787       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0902 05:22:02.864001       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [7ab2e0901743] <==
E0902 05:20:33.617981       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:20:33.617977       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:20:33.617881       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:20:33.617708       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:20:33.617708       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:20:33.617714       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:20:33.617848       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:20:33.618155       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:20:33.618174       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:20:34.436568       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:20:34.533453       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:20:34.574304       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0902 05:20:34.593007       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:20:34.675360       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:20:34.748279       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:20:34.759897       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:20:34.770426       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:20:34.866277       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:20:34.929179       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:20:34.960770       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:20:35.025954       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:20:35.036975       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:20:35.112864       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:20:35.139496       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:20:35.143177       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:20:36.306040       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:20:36.552612       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:20:36.721389       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:20:36.752252       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:20:36.883852       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:20:37.112958       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:20:37.219374       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:20:37.237298       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0902 05:20:37.351595       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:20:37.444162       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:20:37.454073       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:20:37.818021       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:20:37.820560       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:20:37.827166       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:20:37.995189       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:20:38.280006       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0902 05:20:40.058430       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0902 05:20:40.103645       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0902 05:20:40.124797       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0902 05:20:40.335673       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0902 05:20:40.790668       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0902 05:20:40.820325       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0902 05:20:41.237958       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0902 05:20:41.419456       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0902 05:20:41.537708       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0902 05:20:41.889955       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0902 05:20:42.132182       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0902 05:20:43.067943       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0902 05:20:43.259083       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0902 05:20:43.295962       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0902 05:20:43.775122       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0902 05:20:44.214802       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
I0902 05:20:51.516940       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0902 05:20:53.535388       1 server.go:271] "handlers are not fully synchronized" err="context canceled"
E0902 05:20:53.537069       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Sep 02 05:21:25 minikube kubelet[2587]: I0902 05:21:25.563941    2587 status_manager.go:895] "Failed to get status for pod" podUID="4dc72c15-da2d-4e45-bec5-3a1d5c3ce56f" pod="kube-system/kube-proxy-lrjqk" err="Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-lrjqk\": dial tcp 192.168.58.2:8443: connect: connection refused"
Sep 02 05:21:25 minikube kubelet[2587]: I0902 05:21:25.564162    2587 status_manager.go:895] "Failed to get status for pod" podUID="c871e6f7025c72d9d868cfe04d5aa4ac" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Sep 02 05:21:25 minikube kubelet[2587]: I0902 05:21:25.564377    2587 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Sep 02 05:21:25 minikube kubelet[2587]: I0902 05:21:25.564633    2587 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Sep 02 05:21:29 minikube kubelet[2587]: I0902 05:21:29.991435    2587 scope.go:117] "RemoveContainer" containerID="392e4dea16ce1760c3c8d04fa1eb9b487079af802363c2760a7bee7607aaee00"
Sep 02 05:21:33 minikube kubelet[2587]: I0902 05:21:33.991667    2587 scope.go:117] "RemoveContainer" containerID="418ade6445731e57de671a12e251b8bd9f8b4aa6531349559a345661341404f5"
Sep 02 05:21:36 minikube kubelet[2587]: E0902 05:21:36.585200    2587 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"kube-root-ca.crt\"" type="*v1.ConfigMap"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.585562    2587 status_manager.go:895] "Failed to get status for pod" podUID="fe925670c18db101e16e9f2572e23070" pod="kube-system/etcd-minikube" err="pods \"etcd-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.591315    2587 status_manager.go:895] "Failed to get status for pod" podUID="4dc72c15-da2d-4e45-bec5-3a1d5c3ce56f" pod="kube-system/kube-proxy-lrjqk" err="pods \"kube-proxy-lrjqk\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.592098    2587 status_manager.go:895] "Failed to get status for pod" podUID="c871e6f7025c72d9d868cfe04d5aa4ac" pod="kube-system/kube-apiserver-minikube" err="pods \"kube-apiserver-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.592877    2587 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="pods \"kube-scheduler-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.593814    2587 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="pods \"kube-controller-manager-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.596264    2587 status_manager.go:895] "Failed to get status for pod" podUID="7bfc3c58-a679-4f0b-854d-67085ba29c98" pod="kube-system/coredns-674b8bbfcf-2gm4p" err="pods \"coredns-674b8bbfcf-2gm4p\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.621411    2587 status_manager.go:895] "Failed to get status for pod" podUID="0ba344fc-c49d-45a1-9430-c200c23e1f46" pod="kube-system/coredns-674b8bbfcf-qs5rv" err="pods \"coredns-674b8bbfcf-qs5rv\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.622197    2587 status_manager.go:895] "Failed to get status for pod" podUID="894623a4-ee12-4be3-a8b1-9fdaeb91cf0b" pod="kube-system/storage-provisioner" err="pods \"storage-provisioner\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.622850    2587 status_manager.go:895] "Failed to get status for pod" podUID="4dc72c15-da2d-4e45-bec5-3a1d5c3ce56f" pod="kube-system/kube-proxy-lrjqk" err="pods \"kube-proxy-lrjqk\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.623838    2587 status_manager.go:895] "Failed to get status for pod" podUID="c871e6f7025c72d9d868cfe04d5aa4ac" pod="kube-system/kube-apiserver-minikube" err="pods \"kube-apiserver-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.627095    2587 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="pods \"kube-scheduler-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.628634    2587 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="pods \"kube-controller-manager-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.629323    2587 status_manager.go:895] "Failed to get status for pod" podUID="7bfc3c58-a679-4f0b-854d-67085ba29c98" pod="kube-system/coredns-674b8bbfcf-2gm4p" err="pods \"coredns-674b8bbfcf-2gm4p\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.632039    2587 status_manager.go:895] "Failed to get status for pod" podUID="0ba344fc-c49d-45a1-9430-c200c23e1f46" pod="kube-system/coredns-674b8bbfcf-qs5rv" err="pods \"coredns-674b8bbfcf-qs5rv\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.633239    2587 status_manager.go:895] "Failed to get status for pod" podUID="894623a4-ee12-4be3-a8b1-9fdaeb91cf0b" pod="kube-system/storage-provisioner" err="pods \"storage-provisioner\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Sep 02 05:21:36 minikube kubelet[2587]: I0902 05:21:36.991196    2587 scope.go:117] "RemoveContainer" containerID="e2914f24829c50ad5aa4dac6e2f00e238c10e081b0c0ff61a2ce6c54958d5fdd"
Sep 02 05:21:36 minikube kubelet[2587]: E0902 05:21:36.991661    2587 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(0378f173c980f85a71d36305bacb0ad1)\"" pod="kube-system/kube-controller-manager-minikube" podUID="0378f173c980f85a71d36305bacb0ad1"
Sep 02 05:21:41 minikube kubelet[2587]: I0902 05:21:41.991893    2587 scope.go:117] "RemoveContainer" containerID="8a0e2b46bc02d30f7026976082a888e4471226851e9ef114750426b9e416cec2"
Sep 02 05:21:41 minikube kubelet[2587]: E0902 05:21:41.992203    2587 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(894623a4-ee12-4be3-a8b1-9fdaeb91cf0b)\"" pod="kube-system/storage-provisioner" podUID="894623a4-ee12-4be3-a8b1-9fdaeb91cf0b"
Sep 02 05:21:50 minikube kubelet[2587]: I0902 05:21:50.991989    2587 scope.go:117] "RemoveContainer" containerID="e2914f24829c50ad5aa4dac6e2f00e238c10e081b0c0ff61a2ce6c54958d5fdd"
Sep 02 05:21:50 minikube kubelet[2587]: E0902 05:21:50.992216    2587 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(0378f173c980f85a71d36305bacb0ad1)\"" pod="kube-system/kube-controller-manager-minikube" podUID="0378f173c980f85a71d36305bacb0ad1"
Sep 02 05:21:55 minikube kubelet[2587]: I0902 05:21:55.991876    2587 scope.go:117] "RemoveContainer" containerID="8a0e2b46bc02d30f7026976082a888e4471226851e9ef114750426b9e416cec2"
Sep 02 05:21:55 minikube kubelet[2587]: E0902 05:21:55.992070    2587 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(894623a4-ee12-4be3-a8b1-9fdaeb91cf0b)\"" pod="kube-system/storage-provisioner" podUID="894623a4-ee12-4be3-a8b1-9fdaeb91cf0b"
Sep 02 05:22:02 minikube kubelet[2587]: I0902 05:22:02.919772    2587 scope.go:117] "RemoveContainer" containerID="e2914f24829c50ad5aa4dac6e2f00e238c10e081b0c0ff61a2ce6c54958d5fdd"
Sep 02 05:22:09 minikube kubelet[2587]: I0902 05:22:09.919779    2587 scope.go:117] "RemoveContainer" containerID="8a0e2b46bc02d30f7026976082a888e4471226851e9ef114750426b9e416cec2"
Sep 02 05:32:17 minikube kubelet[2587]: I0902 05:32:17.387255    2587 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6gkkg\" (UniqueName: \"kubernetes.io/projected/b7db6177-d405-40a7-99fa-0ea87fcd1493-kube-api-access-6gkkg\") pod \"travel-api-deployment-ccd8bf76-sc866\" (UID: \"b7db6177-d405-40a7-99fa-0ea87fcd1493\") " pod="new-travel-api/travel-api-deployment-ccd8bf76-sc866"
Sep 02 05:32:44 minikube kubelet[2587]: I0902 05:32:44.726394    2587 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="new-travel-api/travel-api-deployment-ccd8bf76-sc866" podStartSLOduration=3.022887116 podStartE2EDuration="27.725121665s" podCreationTimestamp="2025-09-02 05:32:17 +0000 UTC" firstStartedPulling="2025-09-02 05:32:18.135114561 +0000 UTC m=+1057.882553682" lastFinishedPulling="2025-09-02 05:32:43.593202524 +0000 UTC m=+1082.584788231" observedRunningTime="2025-09-02 05:32:44.713188758 +0000 UTC m=+1083.704774474" watchObservedRunningTime="2025-09-02 05:32:44.725121665 +0000 UTC m=+1083.716707371"
Sep 02 05:44:41 minikube kubelet[2587]: I0902 05:44:41.673299    2587 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-89cs5\" (UniqueName: \"kubernetes.io/projected/e85edd00-109b-4cfd-aaf5-dd7b22b85ade-kube-api-access-89cs5\") pod \"ingress-nginx-admission-create-tmcnp\" (UID: \"e85edd00-109b-4cfd-aaf5-dd7b22b85ade\") " pod="ingress-nginx/ingress-nginx-admission-create-tmcnp"
Sep 02 05:44:41 minikube kubelet[2587]: I0902 05:44:41.673506    2587 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7vrsx\" (UniqueName: \"kubernetes.io/projected/54f4e77b-bb62-4890-bd51-a60cad1d3919-kube-api-access-7vrsx\") pod \"ingress-nginx-controller-67c5cb88f-xnm5d\" (UID: \"54f4e77b-bb62-4890-bd51-a60cad1d3919\") " pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-xnm5d"
Sep 02 05:44:41 minikube kubelet[2587]: I0902 05:44:41.673526    2587 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qfxzq\" (UniqueName: \"kubernetes.io/projected/fc1bef08-0dc1-446d-ac60-34188a7fa2bf-kube-api-access-qfxzq\") pod \"ingress-nginx-admission-patch-j6j79\" (UID: \"fc1bef08-0dc1-446d-ac60-34188a7fa2bf\") " pod="ingress-nginx/ingress-nginx-admission-patch-j6j79"
Sep 02 05:44:41 minikube kubelet[2587]: I0902 05:44:41.673541    2587 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert\") pod \"ingress-nginx-controller-67c5cb88f-xnm5d\" (UID: \"54f4e77b-bb62-4890-bd51-a60cad1d3919\") " pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-xnm5d"
Sep 02 05:44:41 minikube kubelet[2587]: E0902 05:44:41.777886    2587 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Sep 02 05:44:41 minikube kubelet[2587]: E0902 05:44:41.779249    2587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert podName:54f4e77b-bb62-4890-bd51-a60cad1d3919 nodeName:}" failed. No retries permitted until 2025-09-02 05:44:42.278246253 +0000 UTC m=+1780.949305847 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-xnm5d" (UID: "54f4e77b-bb62-4890-bd51-a60cad1d3919") : secret "ingress-nginx-admission" not found
Sep 02 05:44:42 minikube kubelet[2587]: E0902 05:44:42.278392    2587 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Sep 02 05:44:42 minikube kubelet[2587]: E0902 05:44:42.278485    2587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert podName:54f4e77b-bb62-4890-bd51-a60cad1d3919 nodeName:}" failed. No retries permitted until 2025-09-02 05:44:43.278470742 +0000 UTC m=+1781.949530336 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-xnm5d" (UID: "54f4e77b-bb62-4890-bd51-a60cad1d3919") : secret "ingress-nginx-admission" not found
Sep 02 05:44:42 minikube kubelet[2587]: I0902 05:44:42.345213    2587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="71e88c220c9f9d08c7c52f97240c509bed3a8756e2118d04bf37c610d3b88289"
Sep 02 05:44:42 minikube kubelet[2587]: I0902 05:44:42.347227    2587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="957542b427118f4bfcfbecab31bc4b10543a049af97f41e7fee2f2bd7513e0e6"
Sep 02 05:44:43 minikube kubelet[2587]: E0902 05:44:43.286738    2587 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Sep 02 05:44:43 minikube kubelet[2587]: E0902 05:44:43.286834    2587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert podName:54f4e77b-bb62-4890-bd51-a60cad1d3919 nodeName:}" failed. No retries permitted until 2025-09-02 05:44:45.286815629 +0000 UTC m=+1783.957875223 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-xnm5d" (UID: "54f4e77b-bb62-4890-bd51-a60cad1d3919") : secret "ingress-nginx-admission" not found
Sep 02 05:44:45 minikube kubelet[2587]: E0902 05:44:45.301374    2587 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Sep 02 05:44:45 minikube kubelet[2587]: E0902 05:44:45.301486    2587 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert podName:54f4e77b-bb62-4890-bd51-a60cad1d3919 nodeName:}" failed. No retries permitted until 2025-09-02 05:44:49.301464542 +0000 UTC m=+1787.972524146 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/54f4e77b-bb62-4890-bd51-a60cad1d3919-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-xnm5d" (UID: "54f4e77b-bb62-4890-bd51-a60cad1d3919") : secret "ingress-nginx-admission" not found
Sep 02 05:44:47 minikube kubelet[2587]: I0902 05:44:47.432923    2587 scope.go:117] "RemoveContainer" containerID="fbeb27375fe624409b30f41792be65805d60cef3a1686a439d044457b6b6db19"
Sep 02 05:44:48 minikube kubelet[2587]: I0902 05:44:48.461814    2587 scope.go:117] "RemoveContainer" containerID="fbeb27375fe624409b30f41792be65805d60cef3a1686a439d044457b6b6db19"
Sep 02 05:44:48 minikube kubelet[2587]: I0902 05:44:48.928331    2587 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-89cs5\" (UniqueName: \"kubernetes.io/projected/e85edd00-109b-4cfd-aaf5-dd7b22b85ade-kube-api-access-89cs5\") pod \"e85edd00-109b-4cfd-aaf5-dd7b22b85ade\" (UID: \"e85edd00-109b-4cfd-aaf5-dd7b22b85ade\") "
Sep 02 05:44:48 minikube kubelet[2587]: I0902 05:44:48.934598    2587 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e85edd00-109b-4cfd-aaf5-dd7b22b85ade-kube-api-access-89cs5" (OuterVolumeSpecName: "kube-api-access-89cs5") pod "e85edd00-109b-4cfd-aaf5-dd7b22b85ade" (UID: "e85edd00-109b-4cfd-aaf5-dd7b22b85ade"). InnerVolumeSpecName "kube-api-access-89cs5". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Sep 02 05:44:49 minikube kubelet[2587]: I0902 05:44:49.029690    2587 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-89cs5\" (UniqueName: \"kubernetes.io/projected/e85edd00-109b-4cfd-aaf5-dd7b22b85ade-kube-api-access-89cs5\") on node \"minikube\" DevicePath \"\""
Sep 02 05:44:49 minikube kubelet[2587]: I0902 05:44:49.596967    2587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2883c667f131613a9f23dc21477eec22948783959d7787a829d41041d112a7c7"
Sep 02 05:44:49 minikube kubelet[2587]: I0902 05:44:49.610576    2587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="957542b427118f4bfcfbecab31bc4b10543a049af97f41e7fee2f2bd7513e0e6"
Sep 02 05:44:50 minikube kubelet[2587]: I0902 05:44:50.035329    2587 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qfxzq\" (UniqueName: \"kubernetes.io/projected/fc1bef08-0dc1-446d-ac60-34188a7fa2bf-kube-api-access-qfxzq\") pod \"fc1bef08-0dc1-446d-ac60-34188a7fa2bf\" (UID: \"fc1bef08-0dc1-446d-ac60-34188a7fa2bf\") "
Sep 02 05:44:50 minikube kubelet[2587]: I0902 05:44:50.038856    2587 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fc1bef08-0dc1-446d-ac60-34188a7fa2bf-kube-api-access-qfxzq" (OuterVolumeSpecName: "kube-api-access-qfxzq") pod "fc1bef08-0dc1-446d-ac60-34188a7fa2bf" (UID: "fc1bef08-0dc1-446d-ac60-34188a7fa2bf"). InnerVolumeSpecName "kube-api-access-qfxzq". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Sep 02 05:44:50 minikube kubelet[2587]: I0902 05:44:50.135862    2587 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-qfxzq\" (UniqueName: \"kubernetes.io/projected/fc1bef08-0dc1-446d-ac60-34188a7fa2bf-kube-api-access-qfxzq\") on node \"minikube\" DevicePath \"\""
Sep 02 05:44:50 minikube kubelet[2587]: I0902 05:44:50.629432    2587 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="71e88c220c9f9d08c7c52f97240c509bed3a8756e2118d04bf37c610d3b88289"
Sep 02 05:45:02 minikube kubelet[2587]: I0902 05:45:02.795794    2587 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-xnm5d" podStartSLOduration=10.222599091 podStartE2EDuration="21.792930938s" podCreationTimestamp="2025-09-02 05:44:41 +0000 UTC" firstStartedPulling="2025-09-02 05:44:49.914676302 +0000 UTC m=+1788.585735895" lastFinishedPulling="2025-09-02 05:45:01.485008139 +0000 UTC m=+1800.156067742" observedRunningTime="2025-09-02 05:45:02.792658987 +0000 UTC m=+1801.463718580" watchObservedRunningTime="2025-09-02 05:45:02.792930938 +0000 UTC m=+1801.463990532"


==> storage-provisioner [06cca6cc885d] <==
W0902 05:55:06.851991       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:06.857597       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:09.383633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:09.398243       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:11.401600       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:11.405648       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:13.408622       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:13.423626       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:15.426761       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:15.432644       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:17.435283       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:17.440003       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:19.443635       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:19.448170       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:21.451945       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:21.465546       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:23.467724       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:23.474579       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:25.477954       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:25.492805       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:27.496166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:27.512335       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:29.515057       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:29.518876       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:31.521669       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:31.537680       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:33.541209       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:33.555406       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:35.558049       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:35.564555       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:37.577315       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:37.585074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:39.588099       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:39.596906       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:42.173762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:42.192981       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:44.196604       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:44.211958       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:46.214876       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:46.221283       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:48.224524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:48.240092       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:50.243217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:50.250493       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:52.253852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:52.259603       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:54.262817       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:54.271126       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:56.275046       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:56.279560       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:58.283157       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:55:58.300607       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:00.304205       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:00.309500       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:02.312855       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:02.317472       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:04.320156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:04.323988       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:06.327041       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0902 05:56:06.333870       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [8a0e2b46bc02] <==
I0902 05:21:25.247218       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0902 05:21:25.252073       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

